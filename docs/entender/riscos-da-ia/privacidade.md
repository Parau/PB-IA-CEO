---
sidebar_position: 4
---
import LigaHighlight from '@site/src/components/gsap/highlight'
import BoasVindasChatGPT from '@site/src/components/BoasVindasChatGPT'

# Privacidade
<LigaHighlight />
No cenário atual, a conformidade com a <spam class="text-highlight">Lei Geral de Proteção de Dados</spam> (LGPD) é uma necessidade inegociável para qualquer organização que deseja operar de maneira <spam class="text-highlight">ética e segura</spam>. No entanto, a implementação de IA adiciona uma camada extra de complexidade a essa responsabilidade.

O que você deve observar?

## Uso de ferramentas prontas
Ao implantar ferramentas de IA prontas em sua organização, é importante estar atento a como essas plataformas lidam com os dados dos usuários. Muitas vezes, essas ferramentas <spam class="text-highlight">armazenam informações</spam> para auditoria e melhoria contínua dos modelos, o que pode levantar preocupações sobre a segurança e o uso dos dados. Portanto, é essencial verificar se a ferramenta de IA está <spam class="text-highlight">em conformidade</spam> com as políticas de privacidade da sua instituição e com a LGPD.

Além da proteção de dados, é importante considerar <spam class="text-highlight">os riscos</spam> associados ao uso de dados sigilosos da organização nessas ferramentas. Por exemplo, enviar <spam class="text-highlight">dados confidenciais</spam> sobre colaboradores para <spam class="text-highlight">gerar relatórios</spam>, ou utilizar informações dos clientes para criar textos personalizados, pode resultar em exposições indesejadas. Um exemplo notável é o da Samsung, que proibiu temporariamente o uso do ChatGPT por seus funcionários em dispositivos de trabalho e redes internas devido a preocupações com privacidade e segurança. Colaboradores da Samsung usaram o ChatGPT para revisar código-fonte, otimizar software e redigir atas de reuniões, o que resultou na <spam class="text-highlight">exposição de informações confidenciais da organização.</spam>

<BoasVindasChatGPT />
 
É importante destacar que o ChatGPT, assim como outras ferramentas de IA generativa, <spam class="text-highlight">pode usar os dados</spam> que você envia para aprimorar seus modelos. Embora o ChatGPT ofereça opções de privacidade para controlar as informações compartilhadas, é fundamental estar atento ao tipo de dados transmitidos e como são utilizados. O compartilhamento de informações confidenciais pode acarretar riscos futuros, especialmente se esses dados forem <spam class="text-highlight">integrados inadvertidamente ao modelo.</spam>

Lembre-se de que a LGPD exige que o tratamento de dados pessoais seja realizado de forma rigorosamente confidencial. É fundamental que a sua organização implemente medidas técnicas e organizacionais adequadas para proteger esses dados contra acessos não autorizados, vazamentos, e outras formas de uso indevido.

## Treinando os seus próprios modelos de IA
Para organizaçoes que treinam ou personalizam seus próprios modelos de IA, a questão da privacidade envolve o armazenamento e o processamento dos dados utilizados no treinamento. É essencial <spam class="text-highlight">implementar medidas rigorosas</spam> para proteger esses dados contra acessos não autorizados e vazamentos. Além disso, vale considerar a adoção de técnicas como <spam class="text-highlight">anonimização e pseudonimização de dados</spam>, que podem reduzir significativamente os riscos associados ao processamento de informações sensíveis. Essas técnicas ajudam a proteger a identidade dos indivíduos, dificultando a reidentificação dos dados pessoais em caso de vazamento ou acesso indevido.

## Controle de acesso
Especialmente ao <spam class="text-highlight-end">treinar modelos de linguagem utilizando dados da organização</spam>, é vital estar atento aos níveis de confidencialidade desses dados. Treinar um modelo de linguagem em um grande conjunto de dados, como documentos e relatórios da organização, pode ser útil para diversas tarefas, como gerar resumos, responder a perguntas ou até mesmo criar novos conteúdos. No entanto, é importante lembrar que, se o modelo for treinado com <spam class="text-highlight-end">dados que não consideram os diferentes níveis de acesso dos usuários</spam> dentro da organização, ele pode acabar gerando informações confidenciais que não deveriam ser acessíveis a todos.